{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Resume Data - Entity Extraction and Graph Loading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv('.streamlit/secrets.toml', override=True)\n",
    "\n",
    "# Neo4j\n",
    "NEO4J_URI = os.getenv('RESUME_NEO4J_URI')\n",
    "NEO4J_USERNAME = os.getenv('RESUME_NEO4J_USERNAME')\n",
    "NEO4J_PASSWORD = os.getenv('RESUME_NEO4J_PASSWORD')\n",
    "\n",
    "#OPENAI\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GenAI Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(temperature=0, model_name=\"gpt-4-0125-preview\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert extraction algorithm for resumes from job aspirants. \"\n",
    "            \"Only extract relevant information from the text. \"\n",
    "            \"If you do not know the value of an attribute asked to extract, \"\n",
    "            \"return an empty string, '', for the attribute's value.\"\n",
    "            \"Do not create fictitious data or impute missing values.\"\n",
    "        ),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import datetime\n",
    "from typing import List\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class Position(BaseModel):\n",
    "    id: str = Field(description=\"Unique position id\")\n",
    "    title: str = Field(description=\"The job title\")\n",
    "    location: str = Field(description=\"Location of position\")\n",
    "    startDate: str = Field(description=\"Start date of position\")\n",
    "    endDate: str = Field(description=\"End date of position\")\n",
    "    description: str = Field(description=\"A crisp text summary of position that MUST NOT be more than 100 characters\")\n",
    "    company: str = Field(description=\"Name of company they worked the position for\")\n",
    "\n",
    "class Skill(BaseModel):\n",
    "    id: str = Field(description=\"Unique skill id\")\n",
    "    name: str = Field(description=\"The name of the skill\")\n",
    "    level: str = Field(description=\"Experience level\")\n",
    "\n",
    "class Education(BaseModel):\n",
    "    id: str = Field(description=\"Unique education id\")\n",
    "    degree: str = Field(description=\"Name of educational degree\")\n",
    "    institution: str = Field(description=\"Name of educational institution\")\n",
    "    location: str = Field(description=\"Location of educational institution\")\n",
    "    graduationDate: str = Field(description=\"Date of graduation\")\n",
    "\n",
    "class Person(BaseModel):\n",
    "    id: str = Field(description=\"Unique person id\")\n",
    "    role: str = Field(description=\"The job/employment role\")\n",
    "    description: str = Field(description=\"A crisp text summary and MUST NOT be more than 250 characters\")\n",
    "    positions: List[Position]\n",
    "    skills: List[Skill]\n",
    "    education: List[Education]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zachblumenfeld/opt/anaconda3/envs/genai-workshop/lib/python3.10/site-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The function `with_structured_output` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm.with_structured_output(Person)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neo4j Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.graphs.neo4j_graph import Neo4jGraph\n",
    "\n",
    "graph = Neo4jGraph(NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD)\n",
    "graph.query('CREATE CONSTRAINT person_entityId IF NOT EXISTS FOR (p:Person) REQUIRE (p.entityId) IS UNIQUE;')\n",
    "\n",
    "graph.query('CREATE CONSTRAINT position_entityId IF NOT EXISTS FOR (p:Position) REQUIRE (p.entityId) IS UNIQUE;')\n",
    "graph.query('CREATE CONSTRAINT company_entityId IF NOT EXISTS FOR (p:Company) REQUIRE (p.entityId) IS UNIQUE;')\n",
    "graph.query('CREATE CONSTRAINT job_title_entityId IF NOT EXISTS FOR (p:JobTitle) REQUIRE (p.entityId) IS UNIQUE;')\n",
    "\n",
    "graph.query('CREATE CONSTRAINT skill_entityId IF NOT EXISTS FOR (p:Skill) REQUIRE (p.entityId) IS UNIQUE;')\n",
    "\n",
    "graph.query('CREATE CONSTRAINT education_entityId IF NOT EXISTS FOR (p:Education) REQUIRE (p.entityId) IS UNIQUE;')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extraction & Loading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "TOTAL_DOCUMENTS = 4\n",
    "LOAD_CHUNK_SIZE = 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "\n",
    "def clean_text(t):\n",
    "    return re.sub(r'[^\\x00-\\x7F]+',' ', t)\n",
    "\n",
    "def chunks(xs, n: int = 1_000):\n",
    "    n = max(1, n)\n",
    "    return [xs[i:i + n] for i in range(0, len(xs), n)]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def format_list_dict(objs: BaseModel, source_id):\n",
    "    res = []\n",
    "    for obj in objs:\n",
    "        d = obj.dict()\n",
    "        d['sourceId'] =  source_id\n",
    "        res.append(d)\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def extract_data(txt_files):\n",
    "    people = []\n",
    "    positions = []\n",
    "    skills = []\n",
    "    educations = []\n",
    "    failed_files = []\n",
    "\n",
    "    for i in range(len(txt_files)):\n",
    "        with open(txt_files[i], 'r', encoding='utf-8', errors='ignore') as file:\n",
    "            text = clean_text(file.read().rstrip())\n",
    "            try:\n",
    "                person = chain.invoke(text)\n",
    "                people.append({\n",
    "                    'id':person.id,\n",
    "                    'role': person.role,\n",
    "                    'description': person.description,\n",
    "                    'sourceId': txt_files[i]\n",
    "                })\n",
    "                positions.extend(format_list_dict(person.positions, txt_files[i]))\n",
    "                skills.extend(format_list_dict(person.skills, txt_files[i]))\n",
    "                educations.extend(format_list_dict(person.education, txt_files[i]))\n",
    "                print(f\"Successfully processed {i+1} of {len(txt_files)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"{txt_files[i]}: Processing Failed with exception {e}\")\n",
    "                failed_files.append(txt_files[i])\n",
    "    return people, positions, skills, educations, failed_files\n",
    "\n",
    "def load_data(people, positions, skills, educations):\n",
    "    for recs in chunks(people):\n",
    "        graph.query('''\n",
    "        UNWIND $recs AS rec\n",
    "        MERGE(n:Person {entityId: rec.sourceId})\n",
    "        SET n += rec\n",
    "        RETURN count(n)\n",
    "        ''', params={'recs': recs})\n",
    "\n",
    "    for recs in chunks(positions):\n",
    "        graph.query('''\n",
    "        UNWIND $recs AS rec\n",
    "        MATCH(p:Person {entityId: rec.sourceId})\n",
    "        MERGE(n:Position {entityId: rec.sourceId + ' - ' + rec.id})\n",
    "        SET n += rec\n",
    "        MERGE(p)-[:HAS_POSITION]->(n)\n",
    "        WITH n\n",
    "        MERGE(j:JobTitle {entityId: toUpper(n.title)})\n",
    "        MERGE(n)-[r:WITH_TITLE]->(j)\n",
    "        WITH n\n",
    "        WHERE n.company <> \"\"\n",
    "        MERGE(c:Company {entityId: toUpper(n.company)})\n",
    "        MERGE(n)-[r:AT_COMPANY]->(c)\n",
    "        RETURN count(n)\n",
    "        ''', params={'recs': recs})\n",
    "    for recs in chunks(skills):\n",
    "        graph.query('''\n",
    "        UNWIND $recs AS rec\n",
    "        MATCH(p:Person {entityId: rec.sourceId})\n",
    "        MERGE(n:Skill {entityId: toUpper(rec.name)})\n",
    "        MERGE(p)-[r:HAS_SKILL]->(n)\n",
    "        SET r += rec\n",
    "        RETURN count(r)\n",
    "        ''', params={'recs': recs})\n",
    "    for recs in chunks(educations):\n",
    "        graph.query('''\n",
    "        UNWIND $recs AS rec\n",
    "        MATCH(p:Person {entityId: rec.sourceId})\n",
    "        MERGE(n:Education {entityId: rec.sourceId + ' - ' + rec.id})\n",
    "        MERGE(p)-[:HAS_EDUCATION]->(n)\n",
    "        SET n += rec\n",
    "        RETURN count(n)\n",
    "        ''', params={'recs': recs})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Extracting Data From Files Segment ========\n",
      "Successfully processed 0 of 2\n",
      "Successfully processed 1 of 2\n",
      "Completed Extraction From Files Segment\n",
      "======= Loading Extracted Data ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:22<00:22, 22.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Extracting Data From Files Segment ========\n",
      "Successfully processed 0 of 2\n",
      "Successfully processed 1 of 2\n",
      "Completed Extraction From Files Segment\n",
      "======= Loading Extracted Data ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:07<00:00, 33.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 98.5 ms, sys: 15.5 ms, total: 114 ms\n",
      "Wall time: 1min 7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "text_files = glob.glob(\"data/*.txt\")[:TOTAL_DOCUMENTS]\n",
    "failed_files_list = []\n",
    "\n",
    "for txt_file_seg in tqdm(chunks(text_files, LOAD_CHUNK_SIZE)):\n",
    "    print('======= Extracting Data From Files Segment ========')\n",
    "    people, positions, skills, educations, failed_files = extract_data(txt_file_seg)\n",
    "    print('Completed Extraction From Files Segment')\n",
    "    failed_files_list.extend(failed_files)\n",
    "    print('======= Loading Extracted Data ========')\n",
    "    load_data(people, positions, skills, educations)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
