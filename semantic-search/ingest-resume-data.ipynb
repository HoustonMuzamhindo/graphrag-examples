{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Resume Data - Entity Extraction and Graph Loading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv('.streamlit/secrets.toml', override=True)\n",
    "\n",
    "# Neo4j\n",
    "NEO4J_URI = os.getenv('RESUME_NEO4J_URI')\n",
    "NEO4J_USERNAME = os.getenv('RESUME_NEO4J_USERNAME')\n",
    "NEO4J_PASSWORD = os.getenv('RESUME_NEO4J_PASSWORD')\n",
    "\n",
    "#OPENAI\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GenAI Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(temperature=0, model_name=\"gpt-4-0125-preview\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert extraction algorithm for resumes from job aspirants. \"\n",
    "            \"Only extract relevant information from the text. \"\n",
    "            \"If you do not know the value of an attribute asked to extract, \"\n",
    "            \"return an empty string, '', for the attribute's value.\"\n",
    "            \"Do not create fictitious data or impute missing values.\"\n",
    "        ),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import datetime\n",
    "from typing import List\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class Position(BaseModel):\n",
    "    id: str = Field(description=\"Unique position id\")\n",
    "    title: str = Field(description=\"The job title\")\n",
    "    location: str = Field(description=\"Location of position\")\n",
    "    startDate: str = Field(description=\"Start date of position\")\n",
    "    endDate: str = Field(description=\"End date of position\")\n",
    "    description: str = Field(description=\"A crisp text summary of position that MUST NOT be more than 100 characters\")\n",
    "    company: str = Field(description=\"Name of company they worked the position for\")\n",
    "\n",
    "class Skill(BaseModel):\n",
    "    id: str = Field(description=\"Unique skill id\")\n",
    "    name: str = Field(description=\"The name of the skill\")\n",
    "    level: str = Field(description=\"Experience level\")\n",
    "\n",
    "class Education(BaseModel):\n",
    "    id: str = Field(description=\"Unique education id\")\n",
    "    degree: str = Field(description=\"Name of educational degree\")\n",
    "    institution: str = Field(description=\"Name of educational institution\")\n",
    "    location: str = Field(description=\"Location of educational institution\")\n",
    "    graduationDate: str = Field(description=\"Date of graduation\")\n",
    "\n",
    "class Person(BaseModel):\n",
    "    id: str = Field(description=\"Unique person id\")\n",
    "    role: str = Field(description=\"The job/employment role\")\n",
    "    description: str = Field(description=\"A crisp text summary and MUST NOT be more than 250 characters\")\n",
    "    positions: List[Position]\n",
    "    skills: List[Skill]\n",
    "    education: List[Education]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zachblumenfeld/opt/anaconda3/envs/genai-workshop/lib/python3.10/site-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The function `with_structured_output` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm.with_structured_output(Person)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neo4j Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.graphs.neo4j_graph import Neo4jGraph\n",
    "\n",
    "graph = Neo4jGraph(NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD)\n",
    "graph.query('CREATE CONSTRAINT person_entityId IF NOT EXISTS FOR (p:Person) REQUIRE (p.entityId) IS UNIQUE;')\n",
    "\n",
    "graph.query('CREATE CONSTRAINT position_entityId IF NOT EXISTS FOR (p:Position) REQUIRE (p.entityId) IS UNIQUE;')\n",
    "graph.query('CREATE CONSTRAINT company_entityId IF NOT EXISTS FOR (p:Company) REQUIRE (p.entityId) IS UNIQUE;')\n",
    "graph.query('CREATE CONSTRAINT job_title_entityId IF NOT EXISTS FOR (p:JobTitle) REQUIRE (p.entityId) IS UNIQUE;')\n",
    "\n",
    "graph.query('CREATE CONSTRAINT skill_entityId IF NOT EXISTS FOR (p:Skill) REQUIRE (p.entityId) IS UNIQUE;')\n",
    "\n",
    "graph.query('CREATE CONSTRAINT education_entityId IF NOT EXISTS FOR (p:Education) REQUIRE (p.entityId) IS UNIQUE;')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extraction & Loading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "TOTAL_DOCUMENTS = 200\n",
    "LOAD_CHUNK_SIZE = 20"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "\n",
    "def clean_text(t):\n",
    "    return re.sub(r'[^\\x00-\\x7F]+',' ', t)\n",
    "\n",
    "def chunks(xs, n: int = 1_000):\n",
    "    n = max(1, n)\n",
    "    return [xs[i:i + n] for i in range(0, len(xs), n)]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def format_list_dict(objs: BaseModel, source_id):\n",
    "    res = []\n",
    "    for obj in objs:\n",
    "        d = obj.dict()\n",
    "        d['sourceId'] =  source_id\n",
    "        res.append(d)\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def extract_data(txt_files):\n",
    "    people = []\n",
    "    positions = []\n",
    "    skills = []\n",
    "    educations = []\n",
    "    failed_files = []\n",
    "\n",
    "    for i in range(len(txt_files)):\n",
    "        with open(txt_files[i], 'r', encoding='utf-8', errors='ignore') as file:\n",
    "            text = clean_text(file.read().rstrip())\n",
    "            try:\n",
    "                person = chain.invoke(text)\n",
    "                people.append({\n",
    "                    'id':person.id,\n",
    "                    'role': person.role,\n",
    "                    'description': person.description,\n",
    "                    'sourceId': txt_files[i]\n",
    "                })\n",
    "                positions.extend(format_list_dict(person.positions, txt_files[i]))\n",
    "                skills.extend(format_list_dict(person.skills, txt_files[i]))\n",
    "                educations.extend(format_list_dict(person.education, txt_files[i]))\n",
    "                print(f\"Successfully processed {i+1} of {len(txt_files)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"{txt_files[i]}: Processing Failed with exception {e}\")\n",
    "                failed_files.append(txt_files[i])\n",
    "    return people, positions, skills, educations, failed_files\n",
    "\n",
    "def load_data(people, positions, skills, educations):\n",
    "    for recs in chunks(people):\n",
    "        graph.query('''\n",
    "        UNWIND $recs AS rec\n",
    "        MERGE(n:Person {entityId: rec.sourceId})\n",
    "        SET n += rec\n",
    "        RETURN count(n)\n",
    "        ''', params={'recs': recs})\n",
    "\n",
    "    for recs in chunks(positions):\n",
    "        graph.query('''\n",
    "        UNWIND $recs AS rec\n",
    "        MATCH(p:Person {entityId: rec.sourceId})\n",
    "        MERGE(n:Position {entityId: rec.sourceId + ' - ' + rec.id})\n",
    "        SET n += rec\n",
    "        MERGE(p)-[:HAS_POSITION]->(n)\n",
    "        WITH n\n",
    "        MERGE(j:JobTitle {entityId: toUpper(n.title)})\n",
    "        MERGE(n)-[r:WITH_TITLE]->(j)\n",
    "        WITH n\n",
    "        WHERE n.company <> \"\"\n",
    "        MERGE(c:Company {entityId: toUpper(n.company)})\n",
    "        MERGE(n)-[r:AT_COMPANY]->(c)\n",
    "        RETURN count(n)\n",
    "        ''', params={'recs': recs})\n",
    "    for recs in chunks(skills):\n",
    "        graph.query('''\n",
    "        UNWIND $recs AS rec\n",
    "        MATCH(p:Person {entityId: rec.sourceId})\n",
    "        MERGE(n:Skill {entityId: toUpper(rec.name)})\n",
    "        MERGE(p)-[r:HAS_SKILL]->(n)\n",
    "        SET r += rec\n",
    "        RETURN count(r)\n",
    "        ''', params={'recs': recs})\n",
    "    for recs in chunks(educations):\n",
    "        graph.query('''\n",
    "        UNWIND $recs AS rec\n",
    "        MATCH(p:Person {entityId: rec.sourceId})\n",
    "        MERGE(n:Education {entityId: rec.sourceId + ' - ' + rec.id})\n",
    "        MERGE(p)-[:HAS_EDUCATION]->(n)\n",
    "        SET n += rec\n",
    "        RETURN count(n)\n",
    "        ''', params={'recs': recs})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Extracting Data From Files Segment ========\n",
      "Successfully processed 1 of 20\n",
      "Successfully processed 2 of 20\n",
      "Successfully processed 3 of 20\n",
      "Successfully processed 4 of 20\n",
      "Successfully processed 5 of 20\n",
      "Successfully processed 6 of 20\n",
      "Successfully processed 7 of 20\n",
      "data/09558.txt: Processing Failed with exception 1 validation error for Person\n",
      "education -> 1 -> location\n",
      "  field required (type=value_error.missing)\n",
      "Successfully processed 9 of 20\n",
      "Successfully processed 10 of 20\n",
      "Successfully processed 11 of 20\n",
      "Successfully processed 12 of 20\n",
      "Successfully processed 13 of 20\n",
      "Successfully processed 14 of 20\n",
      "Successfully processed 15 of 20\n",
      "Successfully processed 16 of 20\n",
      "Successfully processed 17 of 20\n",
      "Successfully processed 18 of 20\n",
      "Successfully processed 19 of 20\n",
      "Successfully processed 20 of 20\n",
      "Completed Extraction From Files Segment\n",
      "======= Loading Extracted Data ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [07:54<1:11:11, 474.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Extracting Data From Files Segment ========\n",
      "Successfully processed 1 of 20\n",
      "Successfully processed 2 of 20\n",
      "Successfully processed 3 of 20\n",
      "Successfully processed 4 of 20\n",
      "Successfully processed 5 of 20\n",
      "Successfully processed 6 of 20\n",
      "Successfully processed 7 of 20\n",
      "Successfully processed 8 of 20\n",
      "Successfully processed 9 of 20\n",
      "Successfully processed 10 of 20\n",
      "Successfully processed 11 of 20\n",
      "Successfully processed 12 of 20\n",
      "Successfully processed 13 of 20\n",
      "Successfully processed 14 of 20\n",
      "Successfully processed 15 of 20\n",
      "Successfully processed 16 of 20\n",
      "Successfully processed 17 of 20\n",
      "Successfully processed 18 of 20\n",
      "Successfully processed 19 of 20\n",
      "Successfully processed 20 of 20\n",
      "Completed Extraction From Files Segment\n",
      "======= Loading Extracted Data ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [14:40<57:51, 433.91s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Extracting Data From Files Segment ========\n",
      "Successfully processed 1 of 20\n",
      "Successfully processed 2 of 20\n",
      "Successfully processed 3 of 20\n",
      "Successfully processed 4 of 20\n",
      "Successfully processed 5 of 20\n",
      "Successfully processed 6 of 20\n",
      "Successfully processed 7 of 20\n",
      "Successfully processed 8 of 20\n",
      "Successfully processed 9 of 20\n",
      "Successfully processed 10 of 20\n",
      "Successfully processed 11 of 20\n",
      "Successfully processed 12 of 20\n",
      "Successfully processed 13 of 20\n",
      "Successfully processed 14 of 20\n",
      "Successfully processed 15 of 20\n",
      "Successfully processed 16 of 20\n",
      "Successfully processed 17 of 20\n",
      "Successfully processed 18 of 20\n",
      "Successfully processed 19 of 20\n",
      "Successfully processed 20 of 20\n",
      "Completed Extraction From Files Segment\n",
      "======= Loading Extracted Data ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [21:36<49:42, 426.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Extracting Data From Files Segment ========\n",
      "Successfully processed 1 of 20\n",
      "Successfully processed 2 of 20\n",
      "Successfully processed 3 of 20\n",
      "Successfully processed 4 of 20\n",
      "Successfully processed 5 of 20\n",
      "Successfully processed 6 of 20\n",
      "Successfully processed 7 of 20\n",
      "Successfully processed 8 of 20\n",
      "Successfully processed 9 of 20\n",
      "Successfully processed 10 of 20\n",
      "Successfully processed 11 of 20\n",
      "Successfully processed 12 of 20\n",
      "Successfully processed 13 of 20\n",
      "Successfully processed 14 of 20\n",
      "Successfully processed 15 of 20\n",
      "Successfully processed 16 of 20\n",
      "Successfully processed 17 of 20\n",
      "Successfully processed 18 of 20\n",
      "Successfully processed 19 of 20\n",
      "Successfully processed 20 of 20\n",
      "Completed Extraction From Files Segment\n",
      "======= Loading Extracted Data ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [28:39<42:28, 424.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Extracting Data From Files Segment ========\n",
      "Successfully processed 1 of 20\n",
      "Successfully processed 2 of 20\n",
      "Successfully processed 3 of 20\n",
      "Successfully processed 4 of 20\n",
      "Successfully processed 5 of 20\n",
      "Successfully processed 6 of 20\n",
      "Successfully processed 7 of 20\n",
      "Successfully processed 8 of 20\n",
      "Successfully processed 9 of 20\n",
      "Successfully processed 10 of 20\n",
      "Successfully processed 11 of 20\n",
      "Successfully processed 12 of 20\n",
      "Successfully processed 13 of 20\n",
      "Successfully processed 14 of 20\n",
      "Successfully processed 15 of 20\n",
      "Successfully processed 16 of 20\n",
      "Successfully processed 17 of 20\n",
      "Successfully processed 18 of 20\n",
      "Successfully processed 19 of 20\n",
      "Successfully processed 20 of 20\n",
      "Completed Extraction From Files Segment\n",
      "======= Loading Extracted Data ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [38:14<39:53, 478.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Extracting Data From Files Segment ========\n",
      "Successfully processed 1 of 20\n",
      "Successfully processed 2 of 20\n",
      "Successfully processed 3 of 20\n",
      "Successfully processed 4 of 20\n",
      "Successfully processed 5 of 20\n",
      "Successfully processed 6 of 20\n",
      "Successfully processed 7 of 20\n",
      "Successfully processed 8 of 20\n",
      "Successfully processed 9 of 20\n",
      "Successfully processed 10 of 20\n",
      "Successfully processed 11 of 20\n",
      "Successfully processed 12 of 20\n",
      "Successfully processed 13 of 20\n",
      "Successfully processed 14 of 20\n",
      "Successfully processed 15 of 20\n",
      "Successfully processed 16 of 20\n",
      "Successfully processed 17 of 20\n",
      "Successfully processed 18 of 20\n",
      "Successfully processed 19 of 20\n",
      "Successfully processed 20 of 20\n",
      "Completed Extraction From Files Segment\n",
      "======= Loading Extracted Data ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [45:42<31:13, 468.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Extracting Data From Files Segment ========\n",
      "Successfully processed 1 of 20\n",
      "Successfully processed 2 of 20\n",
      "Successfully processed 3 of 20\n",
      "Successfully processed 4 of 20\n",
      "Successfully processed 5 of 20\n",
      "Successfully processed 6 of 20\n",
      "Successfully processed 7 of 20\n",
      "Successfully processed 8 of 20\n",
      "Successfully processed 9 of 20\n",
      "Successfully processed 10 of 20\n",
      "Successfully processed 11 of 20\n",
      "Successfully processed 12 of 20\n",
      "Successfully processed 13 of 20\n",
      "Successfully processed 14 of 20\n",
      "Successfully processed 15 of 20\n",
      "Successfully processed 16 of 20\n",
      "Successfully processed 17 of 20\n",
      "Successfully processed 18 of 20\n",
      "Successfully processed 19 of 20\n",
      "Successfully processed 20 of 20\n",
      "Completed Extraction From Files Segment\n",
      "======= Loading Extracted Data ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [53:09<23:04, 461.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Extracting Data From Files Segment ========\n",
      "Successfully processed 1 of 20\n",
      "Successfully processed 2 of 20\n",
      "Successfully processed 3 of 20\n",
      "Successfully processed 4 of 20\n",
      "Successfully processed 5 of 20\n",
      "Successfully processed 6 of 20\n",
      "Successfully processed 7 of 20\n",
      "Successfully processed 8 of 20\n",
      "Successfully processed 9 of 20\n",
      "Successfully processed 10 of 20\n",
      "Successfully processed 11 of 20\n",
      "Successfully processed 12 of 20\n",
      "Successfully processed 13 of 20\n",
      "Successfully processed 14 of 20\n",
      "Successfully processed 15 of 20\n",
      "Successfully processed 16 of 20\n",
      "Successfully processed 17 of 20\n",
      "Successfully processed 18 of 20\n",
      "Successfully processed 19 of 20\n",
      "Successfully processed 20 of 20\n",
      "Completed Extraction From Files Segment\n",
      "======= Loading Extracted Data ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [1:00:01<14:51, 445.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Extracting Data From Files Segment ========\n",
      "Successfully processed 1 of 20\n",
      "Successfully processed 2 of 20\n",
      "Successfully processed 3 of 20\n",
      "Successfully processed 4 of 20\n",
      "Successfully processed 5 of 20\n",
      "Successfully processed 6 of 20\n",
      "Successfully processed 7 of 20\n",
      "Successfully processed 8 of 20\n",
      "Successfully processed 9 of 20\n",
      "Successfully processed 10 of 20\n",
      "Successfully processed 11 of 20\n",
      "Successfully processed 12 of 20\n",
      "Successfully processed 13 of 20\n",
      "Successfully processed 14 of 20\n",
      "Successfully processed 15 of 20\n",
      "Successfully processed 16 of 20\n",
      "Successfully processed 17 of 20\n",
      "Successfully processed 18 of 20\n",
      "Successfully processed 19 of 20\n",
      "Successfully processed 20 of 20\n",
      "Completed Extraction From Files Segment\n",
      "======= Loading Extracted Data ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [1:07:59<07:35, 455.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Extracting Data From Files Segment ========\n",
      "Successfully processed 1 of 20\n",
      "Successfully processed 2 of 20\n",
      "Successfully processed 3 of 20\n",
      "Successfully processed 4 of 20\n",
      "Successfully processed 5 of 20\n",
      "Successfully processed 6 of 20\n",
      "Successfully processed 7 of 20\n",
      "Successfully processed 8 of 20\n",
      "Successfully processed 9 of 20\n",
      "Successfully processed 10 of 20\n",
      "Successfully processed 11 of 20\n",
      "Successfully processed 12 of 20\n",
      "Successfully processed 13 of 20\n",
      "Successfully processed 14 of 20\n",
      "Successfully processed 15 of 20\n",
      "Successfully processed 16 of 20\n",
      "Successfully processed 17 of 20\n",
      "Successfully processed 18 of 20\n",
      "Successfully processed 19 of 20\n",
      "Successfully processed 20 of 20\n",
      "Completed Extraction From Files Segment\n",
      "======= Loading Extracted Data ========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [1:14:59<00:00, 449.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.89 s, sys: 574 ms, total: 4.46 s\n",
      "Wall time: 1h 14min 59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "text_files = glob.glob(\"data/*.txt\")[:TOTAL_DOCUMENTS]\n",
    "failed_files_list = []\n",
    "\n",
    "for txt_file_seg in tqdm(chunks(text_files, LOAD_CHUNK_SIZE)):\n",
    "    print('======= Extracting Data From Files Segment ========')\n",
    "    people, positions, skills, educations, failed_files = extract_data(txt_file_seg)\n",
    "    print('Completed Extraction From Files Segment')\n",
    "    failed_files_list.extend(failed_files)\n",
    "    print('======= Loading Extracted Data ========')\n",
    "    load_data(people, positions, skills, educations)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate Representative Names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "name_prompt_tmpl = \"\"\"\n",
    "Provide a likely name for a person based on the below location info for past education and positions.\n",
    "Just provide the name itself without any additional explanation or text.\n",
    "If you aren't sure of a good name make one up.\n",
    "\n",
    "{info}\n",
    "\"\"\"\n",
    "name_prompt = PromptTemplate.from_template(name_prompt_tmpl)\n",
    "name_llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
    "name_chain = name_prompt | name_llm | StrOutputParser()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'entityId': 'data/08652.txt',\n  'positions': [{'timePeriod': 'July 2012 - Present',\n    'title': 'Sales/ Project Manager/ IT Professional',\n    'location': 'Little Rock, AR'}],\n  'education': [{'degree': 'Bachelors in Management Information Systems',\n    'location': 'Little Rock, AR',\n    'graduationDatee': '2013',\n    'institution': 'University of Arkansas at Little Rock'},\n   {'degree': 'Bachelors in Accounting',\n    'location': 'Little Rock, AR',\n    'graduationDatee': '2013',\n    'institution': 'University of Arkansas at Little Rock'},\n   {'degree': 'Associates of Arts in General',\n    'location': '',\n    'graduationDatee': 'March 2010',\n    'institution': 'Pulaski Technical College'}]},\n {'entityId': 'data/07561.txt',\n  'positions': [{'timePeriod': 'November 2018 - ',\n    'title': 'Project Manager',\n    'location': 'Bloomfield, NJ'},\n   {'timePeriod': 'December 2016 - November 2018',\n    'title': 'IT Support Coordinator',\n    'location': 'Bloomfield, NJ'},\n   {'timePeriod': 'March 2016 - December 2016',\n    'title': 'Office Administrator',\n    'location': 'Ridgewood, NJ'}],\n  'education': [{'degree': 'BA in Clinical Social Work',\n    'location': 'Teaneck, NJ',\n    'graduationDatee': 'September 2016',\n    'institution': 'Fairleigh Dickinson University'}]},\n {'entityId': 'data/08134.txt',\n  'positions': [{'timePeriod': 'November 2018 - ',\n    'title': 'Operations Analyst',\n    'location': 'East Norriton, PA'},\n   {'timePeriod': 'September 2017 - August 2018',\n    'title': 'Sr. IT Project Coordinator - Contractor',\n    'location': ''},\n   {'timePeriod': 'January 2016 - July 2017',\n    'title': 'Transition Knowledge Project Manager',\n    'location': 'Blue Bell, PA'},\n   {'timePeriod': 'November 2014 - July 2015',\n    'title': 'Project Manager, Customer Facing Technology',\n    'location': 'Collegeville, PA'},\n   {'timePeriod': 'October 2011 - October 2014',\n    'title': 'Project Coordinator IT PMO',\n    'location': ''},\n   {'timePeriod': 'June 2008 - October 2011',\n    'title': 'Supervisor, IT Transportation Service Delivery',\n    'location': 'Latin America and Europe'},\n   {'timePeriod': 'March 2007 - June 2008',\n    'title': 'Supervisor IT Client Technology Services',\n    'location': ''},\n   {'timePeriod': 'July 2001 - February 2007',\n    'title': 'Client Technology Service IT Specialist',\n    'location': ''}],\n  'education': [{'degree': None,\n    'location': None,\n    'graduationDatee': None,\n    'institution': None}]}]"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persons_info = graph.query('''\n",
    "MATCH(p:Person)\n",
    "WITH p\n",
    "OPTIONAL MATCH(p)-[:HAS_POSITION]->(s)\n",
    "WITH p, collect({title:s.title, timePeriod:s.startDate + \" - \" + s.endDate, location:s.location}) AS positions\n",
    "OPTIONAL MATCH(p)-[:HAS_EDUCATION]->(e)\n",
    "RETURN p.entityId AS entityId, positions, collect({degree:e.degree, graduationDatee:e.graduationDate,\n",
    "    institution:e.institution, location:e.location}) AS education\n",
    "''')\n",
    "persons_info[:3]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "199"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(persons_info)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199/199 [01:36<00:00,  2.06it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "name_records = []\n",
    "for person_info in tqdm(persons_info):\n",
    "    res = name_chain.invoke(json.dumps(person_info, indent=1))\n",
    "    name_records.append({'entityId':person_info['entityId'], 'name':res})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'entityId': 'data/06643.txt', 'name': 'Samantha Johnson'},\n {'entityId': 'data/09570.txt', 'name': 'Ryan Johnson'},\n {'entityId': 'data/10021.txt', 'name': 'Tyler Johnson'},\n {'entityId': 'data/08108.txt', 'name': 'Ryan Johnson'},\n {'entityId': 'data/06125.txt', 'name': 'Nathan Reynolds'},\n {'entityId': 'data/09216.txt', 'name': 'Nathan Patel'},\n {'entityId': 'data/06131.txt', 'name': 'Natalie Thompson'},\n {'entityId': 'data/09202.txt', 'name': 'Ryan Johnson'},\n {'entityId': 'data/06657.txt', 'name': 'Samantha Rodriguez'},\n {'entityId': 'data/09564.txt', 'name': 'Nathan Washington'},\n {'entityId': 'data/07549.txt', 'name': 'Ryan Sullivan'},\n {'entityId': 'data/10035.txt', 'name': 'Olufemi Adeyemi'},\n {'entityId': 'data/08691.txt', 'name': 'Emily Johnson'},\n {'entityId': 'data/06864.txt', 'name': 'Alexandra Smith'},\n {'entityId': 'data/08849.txt', 'name': 'Nathan Johnson'},\n {'entityId': 'data/08685.txt', 'name': 'Tyler Johnson'},\n {'entityId': 'data/06870.txt', 'name': 'Nkechi Adebowale'},\n {'entityId': 'data/06858.txt', 'name': 'Rajesh Patel'},\n {'entityId': 'data/08875.txt', 'name': 'Nathan Patel'},\n {'entityId': 'data/06680.txt', 'name': 'Ryan Johnson'},\n {'entityId': 'data/08861.txt', 'name': 'Ethan Thompson'},\n {'entityId': 'data/06694.txt', 'name': 'Nathan Patel'},\n {'entityId': 'data/09028.txt', 'name': 'Olumide Adekunle'},\n {'entityId': 'data/07005.txt', 'name': 'Nathan Reynolds'},\n {'entityId': 'data/08336.txt', 'name': 'Michael Johnson'},\n {'entityId': 'data/05612.txt', 'name': 'Ryan Johnson'},\n {'entityId': 'data/09996.txt', 'name': 'Nathan Thompson'},\n {'entityId': 'data/07763.txt', 'name': 'Ryan Johnson'},\n {'entityId': 'data/08450.txt', 'name': 'Samantha Rodriguez'},\n {'entityId': 'data/09982.txt', 'name': 'Ryan Johnson'},\n {'entityId': 'data/07777.txt', 'name': 'Rajesh Patel'},\n {'entityId': 'data/08444.txt', 'name': 'Nathan Johnson'},\n {'entityId': 'data/06469.txt', 'name': 'Samantha Davis'},\n {'entityId': 'data/05606.txt', 'name': 'Samantha Patel'},\n {'entityId': 'data/07011.txt', 'name': 'Nathan Thompson'},\n {'entityId': 'data/08322.txt', 'name': 'Samantha Franklin'},\n {'entityId': 'data/09014.txt', 'name': 'Alex Rodriguez'},\n {'entityId': 'data/06327.txt', 'name': 'Nathan Thompson'},\n {'entityId': 'data/07039.txt', 'name': 'Michael Johnson'},\n {'entityId': 'data/10223.txt', 'name': 'Kwame Mensah'},\n {'entityId': 'data/09772.txt', 'name': 'Ryan Johnson'},\n {'entityId': 'data/06441.txt', 'name': 'Nathan Patel'},\n {'entityId': 'data/07987.txt', 'name': 'Alexandra Smith'},\n {'entityId': 'data/10237.txt', 'name': 'Ryan Johnson'},\n {'entityId': 'data/08478.txt', 'name': 'Nathan Chen'},\n {'entityId': 'data/09766.txt', 'name': 'Ryan Mitchell'},\n {'entityId': 'data/06455.txt', 'name': 'Nathan Thompson'},\n {'entityId': 'data/07993.txt', 'name': 'Nathan Singh'},\n {'entityId': 'data/09000.txt', 'name': 'Natalia Ivanova'},\n {'entityId': 'data/06333.txt', 'name': 'Alexandra Greene'},\n {'entityId': 'data/05809.txt', 'name': 'Samantha Patel'},\n {'entityId': 'data/07978.txt', 'name': 'Nathan Thompson'},\n {'entityId': 'data/09955.txt', 'name': 'Ethan Sullivan'},\n {'entityId': 'data/08493.txt', 'name': 'Samantha Rodriguez'},\n {'entityId': 'data/09799.txt', 'name': 'Alexandra Smith'},\n {'entityId': 'data/09941.txt', 'name': 'Nikhil Patel'},\n {'entityId': 'data/08487.txt', 'name': 'Ryan Sullivan'},\n {'entityId': 'data/05835.txt', 'name': 'Rajesh Patel'},\n {'entityId': 'data/06482.txt', 'name': 'Nathan Thompson'},\n {'entityId': 'data/07944.txt', 'name': 'Nathan Thompson'},\n {'entityId': 'data/09969.txt', 'name': 'Ryan Johnson'},\n {'entityId': 'data/06496.txt', 'name': 'Samantha Rodriguez'},\n {'entityId': 'data/07950.txt', 'name': 'Ryan Johnson'},\n {'entityId': 'data/07788.txt', 'name': 'Samantha Greene'},\n {'entityId': 'data/05821.txt', 'name': 'Rajesh Patel'},\n {'entityId': 'data/07171.txt', 'name': 'Nathan Thompson'},\n {'entityId': 'data/08242.txt', 'name': 'Nathan Reynolds'},\n {'entityId': 'data/05766.txt', 'name': 'Rajesh Kumar'},\n {'entityId': 'data/06509.txt', 'name': 'Nathan Patel'},\n {'entityId': 'data/07617.txt', 'name': 'Rajesh Patel'},\n {'entityId': 'data/08524.txt', 'name': 'Ethan Williams'},\n {'entityId': 'data/07603.txt', 'name': 'Nathan Patel'},\n {'entityId': 'data/08530.txt', 'name': 'Nathan Johnson'},\n {'entityId': 'data/05772.txt', 'name': 'Elena Chen'},\n {'entityId': 'data/07165.txt', 'name': 'Alexandra Smith'},\n {'entityId': 'data/08256.txt', 'name': 'Nathan Patel'},\n {'entityId': 'data/10419.txt', 'name': 'Ryan Sullivan'},\n {'entityId': 'data/09148.txt', 'name': 'Ryan Johnson'},\n {'entityId': 'data/05982.txt', 'name': 'Samantha Patel'},\n {'entityId': 'data/10431.txt', 'name': 'Ryan Johnson'},\n {'entityId': 'data/09160.txt', 'name': 'Nathan Thompson'},\n {'entityId': 'data/06253.txt', 'name': 'Alexandru Popescu'},\n {'entityId': 'data/09606.txt', 'name': 'Nathan Thompson'},\n {'entityId': 'data/06535.txt', 'name': 'Michael Johnson'},\n {'entityId': 'data/10357.txt', 'name': 'Michael Johnson'},\n {'entityId': 'data/08518.txt', 'name': 'Diego da Silva'},\n {'entityId': 'data/09612.txt', 'name': 'Nathan Thompson'},\n {'entityId': 'data/06521.txt', 'name': 'Ryan Johnson'},\n {'entityId': 'data/10343.txt', 'name': 'Ryan Johnson'},\n {'entityId': 'data/10425.txt', 'name': 'Samantha Rodriguez'},\n {'entityId': 'data/07159.txt', 'name': 'Yuki Tanaka'},\n {'entityId': 'data/09174.txt', 'name': 'Ryan Johnson'},\n {'entityId': 'data/06247.txt', 'name': 'Samantha Patel'},\n {'entityId': 'data/05996.txt', 'name': 'Samantha Rodriguez'},\n {'entityId': 'data/08281.txt', 'name': 'Nathan Johnson'},\n {'entityId': 'data/09821.txt', 'name': 'Kwame Mensah'},\n {'entityId': 'data/09835.txt', 'name': 'Nathan Thompson'},\n {'entityId': 'data/07818.txt', 'name': 'Ryan Johnson'},\n {'entityId': 'data/08295.txt', 'name': 'Samantha Rodriguez'},\n {'entityId': 'data/05969.txt', 'name': 'Ryan Johnson'},\n {'entityId': 'data/06290.txt', 'name': 'Samantha Rodriguez'},\n {'entityId': 'data/05941.txt', 'name': 'Samantha Patel'},\n {'entityId': 'data/05799.txt', 'name': 'Ethan Johnson'},\n {'entityId': 'data/10394.txt', 'name': 'Samantha Greene'},\n {'entityId': 'data/07830.txt', 'name': 'Nathan Thompson'},\n {'entityId': 'data/10380.txt', 'name': 'Nathan Reynolds'},\n {'entityId': 'data/09809.txt', 'name': 'Nathan Thompson'},\n {'entityId': 'data/07824.txt', 'name': 'Alex Rodriguez'},\n {'entityId': 'data/05955.txt', 'name': 'Nathan Patel'},\n {'entityId': 'data/06284.txt', 'name': 'Nathan Johnson'},\n {'entityId': 'data/08726.txt', 'name': 'Nathan Chen'},\n {'entityId': 'data/10169.txt', 'name': 'Nathan Johnson'},\n {'entityId': 'data/07415.txt', 'name': 'Nathan Johnson'},\n {'entityId': 'data/09438.txt', 'name': 'Michael Johnson'},\n {'entityId': 'data/05564.txt', 'name': 'Elijah Thompson'},\n {'entityId': 'data/08040.txt', 'name': 'Nathan Thompson'},\n {'entityId': 'data/07373.txt', 'name': 'Maria Rodriguez'},\n {'entityId': 'data/06079.txt', 'name': 'Alexandra Chen'},\n {'entityId': 'data/08054.txt', 'name': 'Ryan Johnson'},\n {'entityId': 'data/07367.txt', 'name': 'Juan Carlos Rodriguez'},\n {'entityId': 'data/05570.txt', 'name': 'Samantha Patel'},\n {'entityId': 'data/08732.txt', 'name': 'Ryan Smith'},\n {'entityId': 'data/07401.txt', 'name': 'Alexandra Smith'},\n {'entityId': 'data/07429.txt', 'name': 'Ji-hoon Kim'},\n {'entityId': 'data/10155.txt', 'name': 'Ryan Johnson'},\n {'entityId': 'data/06737.txt', 'name': 'Samantha Reynolds'},\n {'entityId': 'data/09404.txt', 'name': 'Alexandra Smith'},\n {'entityId': 'data/06051.txt', 'name': 'Alexandra Smith'},\n {'entityId': 'data/09362.txt', 'name': 'Nathan Johnson'},\n {'entityId': 'data/05558.txt', 'name': 'Alex Chang'},\n {'entityId': 'data/06045.txt', 'name': 'Ethan Chen'},\n {'entityId': 'data/09376.txt', 'name': 'Ryan Johnson'},\n {'entityId': 'data/08068.txt', 'name': 'Alexandra Smith'},\n {'entityId': 'data/10141.txt', 'name': 'Nathan Patel'},\n {'entityId': 'data/06723.txt', 'name': 'Ethan Thompson'},\n {'entityId': 'data/09410.txt', 'name': 'Alexandra Reyes'},\n {'entityId': 'data/06910.txt', 'name': 'Elena Patel'},\n {'entityId': 'data/08083.txt', 'name': 'Nathan Thompson'},\n {'entityId': 'data/08097.txt', 'name': 'Tyler Johnson'},\n {'entityId': 'data/09389.txt', 'name': 'Michael Johnson'},\n {'entityId': 'data/08929.txt', 'name': 'Possible name: James Washington'},\n {'entityId': 'data/06904.txt', 'name': 'Ryan Johnson'},\n {'entityId': 'data/08901.txt', 'name': 'Alexandra Smith'},\n {'entityId': 'data/10196.txt', 'name': 'Ryan Johnson'},\n {'entityId': 'data/06092.txt', 'name': 'Natalie Rodriguez'},\n {'entityId': 'data/07398.txt', 'name': 'Alexandra Davis'},\n {'entityId': 'data/06086.txt', 'name': 'Nathan Thompson'},\n {'entityId': 'data/08915.txt', 'name': 'Natalie Thompson'},\n {'entityId': 'data/06938.txt', 'name': 'Ryan Johnson'},\n {'entityId': 'data/10182.txt', 'name': 'Ryan Johnson'},\n {'entityId': 'data/06087.txt', 'name': 'Samantha Rodriguez'},\n {'entityId': 'data/07399.txt', 'name': 'Natalie Jacobs'},\n {'entityId': 'data/10183.txt', 'name': 'Ryan Johnson'},\n {'entityId': 'data/06939.txt', 'name': 'Ryan Johnson'},\n {'entityId': 'data/08914.txt', 'name': 'Nathan Thompson'},\n {'entityId': 'data/10197.txt', 'name': 'Samantha Thompson'},\n {'entityId': 'data/08900.txt', 'name': 'Ryan Johnson'},\n {'entityId': 'data/06093.txt', 'name': 'Samantha Rodriguez'},\n {'entityId': 'data/09388.txt', 'name': 'Jordan Smith'},\n {'entityId': 'data/08096.txt', 'name': \"Ryan O'Malley\"},\n {'entityId': 'data/06905.txt', 'name': 'Samantha Martinez'},\n {'entityId': 'data/08928.txt', 'name': 'Tyler Johnson'},\n {'entityId': 'data/06911.txt', 'name': 'Nathan Thompson'},\n {'entityId': 'data/08082.txt', 'name': 'Samantha Patel'},\n {'entityId': 'data/08069.txt', 'name': 'Tyler Johnson'},\n {'entityId': 'data/09377.txt', 'name': 'Nia Thompson'},\n {'entityId': 'data/06044.txt', 'name': 'Samantha Hayes'},\n {'entityId': 'data/09411.txt', 'name': 'Michael Johnson'},\n {'entityId': 'data/06722.txt', 'name': 'Samantha Johnson'},\n {'entityId': 'data/10140.txt', 'name': 'Ryan Johnson'},\n {'entityId': 'data/09405.txt', 'name': 'Ryan Johnson'},\n {'entityId': 'data/06736.txt', 'name': 'Samantha Greene'},\n {'entityId': 'data/10154.txt', 'name': 'Samantha Greene'},\n {'entityId': 'data/07428.txt', 'name': 'Amit Patel'},\n {'entityId': 'data/05559.txt', 'name': 'Alexandra Smith'},\n {'entityId': 'data/09363.txt', 'name': 'Nathan Johnson'},\n {'entityId': 'data/06050.txt', 'name': 'Samantha Patel'},\n {'entityId': 'data/05571.txt', 'name': 'Nathan Patel'},\n {'entityId': 'data/07366.txt', 'name': 'Michael Johnson'},\n {'entityId': 'data/08055.txt', 'name': 'Nathan Thompson'},\n {'entityId': 'data/06078.txt', 'name': 'Juan Rodriguez'},\n {'entityId': 'data/07400.txt', 'name': 'Nathan Thompson'},\n {'entityId': 'data/08733.txt', 'name': 'Sofia Ramirez'},\n {'entityId': 'data/09439.txt', 'name': 'Nathan Thompson'},\n {'entityId': 'data/07414.txt', 'name': 'Samantha Lee'},\n {'entityId': 'data/08727.txt', 'name': 'Nathan Thompson'},\n {'entityId': 'data/10168.txt', 'name': 'Nathan Johnson'},\n {'entityId': 'data/07372.txt', 'name': 'Samantha Rodriguez'},\n {'entityId': 'data/08041.txt', 'name': 'Ryan Johnson'}]"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_records[10:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'count(n)': 199}]"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.query('''\n",
    "UNWIND $recs AS rec\n",
    "MATCH(n:Person {entityId: rec.entityId})\n",
    "SET n.name = rec.name\n",
    "RETURN count(n)\n",
    "''', params={'recs': name_records})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'entityId': 'data/08652.txt',\n  'name': 'Tyler Johnson',\n  'role': 'Sales/ Project Manager/ IT Professional',\n  'description': 'Experienced in sales, project management, and IT system improvements. Skilled in database development, QAC analysis, and custom software creation.'},\n {'entityId': 'data/07561.txt',\n  'name': 'Samantha Rodriguez',\n  'role': 'Project Manager',\n  'description': 'Seeking a career opportunity that will allow me to utilize my administrative, communication, and problem solving skills.'},\n {'entityId': 'data/08134.txt',\n  'name': 'Natalie Rodriguez',\n  'role': 'Operations Analyst',\n  'description': 'Experienced professional with 18 years supporting and delivering solutions across global operations. Skilled in proposal and contract management, project coordination, and leading cross-functional teams.'}]"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persons_info = graph.query('''\n",
    "MATCH(p:Person)\n",
    "RETURN p.entityId AS entityId, p.name AS name, p.role AS role, p.description AS description\n",
    "''')\n",
    "persons_info[:3]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generate an profile image of this person based on the below data:\n",
      "{\n",
      " \"entityId\": \"data/08652.txt\",\n",
      " \"name\": \"Tyler Johnson\",\n",
      " \"role\": \"Sales/ Project Manager/ IT Professional\",\n",
      " \"description\": \"Experienced in sales, project management, and IT system improvements. Skilled in database development, QAC analysis, and custom software creation.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "img_prompt_tmpl = '''\n",
    "Generate an profile image of this person based on the below data:\n",
    "'''\n",
    "\n",
    "print(img_prompt_tmpl + json.dumps(persons_info[0], indent=1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "if not os.path.exists('img/data'):\n",
    "    os.makedirs('img/data')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/64 [00:13<05:39,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/09404.txt: Processing Failed with exception Error code: 400 - {'error': {'code': 'content_policy_violation', 'message': 'Your request was rejected as a result of our safety system. Your prompt may contain text that is not allowed by our safety system.', 'param': None, 'type': 'invalid_request_error'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [09:59<00:00,  9.37s/it]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "for person_info in tqdm(persons_info[135:]):\n",
    "    try:\n",
    "        response = client.images.generate(\n",
    "            model=\"dall-e-2\",\n",
    "            prompt=img_prompt_tmpl + json.dumps(person_info, indent=1),\n",
    "            size=\"256x256\",\n",
    "            quality=\"standard\",\n",
    "            response_format= \"url\", #\"b64_json\",\n",
    "            n=1,\n",
    "        )\n",
    "\n",
    "        img_data = requests.get(response.data[0].url).content\n",
    "        with open(f'img/{person_info[\"entityId\"][:-4]}.jpg', 'wb') as handler:\n",
    "            handler.write(img_data)\n",
    "    except Exception as e:\n",
    "        print(f\"{person_info['entityId']}: Processing Failed with exception {e}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Vector Properties & Indexes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.query('''\n",
    "MATCH (n:Person) WHERE size(n.description) <> 0\n",
    "WITH collect(n) AS nodes, toInteger(rand()*$numberOfBatches) AS partition\n",
    "CALL {\n",
    "    WITH nodes\n",
    "    CALL genai.vector.encodeBatch([node IN nodes| node.description], \"OpenAI\", { token: $token})\n",
    "    YIELD index, vector\n",
    "    CALL db.create.setNodeVectorProperty(nodes[index], \"textEmbedding\", vector)\n",
    "} IN TRANSACTIONS OF 1 ROW\n",
    "''', params={'token':OPENAI_API_KEY, 'numberOfBatches':8})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dimension = 1536\n",
    "graph.query('''\n",
    "CREATE VECTOR INDEX person_text_embedding IF NOT EXISTS FOR (n:Person) ON (n.textEmbedding)\n",
    "OPTIONS {indexConfig: {\n",
    " `vector.dimensions`: toInteger($dim),\n",
    " `vector.similarity_function`: 'cosine'\n",
    "}}''', params={'dim': embedding_dimension})\n",
    "\n",
    "graph.query('CALL db.awaitIndex(\"person_text_embedding\", 300)')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.query('''\n",
    "MATCH (n:Skill) WHERE size(n.entityId) <> 0\n",
    "WITH collect(n) AS nodes, toInteger(rand()*$numberOfBatches) AS partition\n",
    "CALL {\n",
    "    WITH nodes\n",
    "    CALL genai.vector.encodeBatch([node IN nodes| node.entityId], \"OpenAI\", { token: $token})\n",
    "    YIELD index, vector\n",
    "    CALL db.create.setNodeVectorProperty(nodes[index], \"textEmbedding\", vector)\n",
    "} IN TRANSACTIONS OF 1 ROW\n",
    "''', params={'token':OPENAI_API_KEY, 'numberOfBatches':40})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.query('''\n",
    "CREATE VECTOR INDEX skill_text_embedding IF NOT EXISTS FOR (n:Skill) ON (n.textEmbedding)\n",
    "OPTIONS {indexConfig: {\n",
    " `vector.dimensions`: toInteger($dim),\n",
    " `vector.similarity_function`: 'cosine'\n",
    "}}''', params={'dim': embedding_dimension})\n",
    "\n",
    "graph.query('CALL db.awaitIndex(\"skill_text_embedding\", 300)')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.query('''\n",
    "MATCH (n:Position) WHERE size(n.description) <> 0\n",
    "WITH collect(n) AS nodes, toInteger(rand()*$numberOfBatches) AS partition\n",
    "CALL {\n",
    "    WITH nodes\n",
    "    CALL genai.vector.encodeBatch([node IN nodes| node.description], \"OpenAI\", { token: $token})\n",
    "    YIELD index, vector\n",
    "    CALL db.create.setNodeVectorProperty(nodes[index], \"textEmbedding\", vector)\n",
    "} IN TRANSACTIONS OF 1 ROW\n",
    "''', params={'token':OPENAI_API_KEY, 'numberOfBatches':40})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.query('''\n",
    "CREATE VECTOR INDEX position_text_embedding IF NOT EXISTS FOR (n:Position) ON (n.textEmbedding)\n",
    "OPTIONS {indexConfig: {\n",
    " `vector.dimensions`: toInteger($dim),\n",
    " `vector.similarity_function`: 'cosine'\n",
    "}}''', params={'dim': embedding_dimension})\n",
    "\n",
    "graph.query('CALL db.awaitIndex(\"position_text_embedding\", 300)')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Full Text Embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.query('''\n",
    "CREATE FULLTEXT INDEX person_full_text IF NOT EXISTS FOR (n:Person) ON EACH [n.role]\n",
    "''')\n",
    "\n",
    "graph.query('CALL db.awaitIndex(\"person_full_text\", 300)')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.query('''\n",
    "CREATE FULLTEXT INDEX skill_full_text IF NOT EXISTS FOR (n:Skill) ON EACH [n.entityId]\n",
    "''')\n",
    "\n",
    "graph.query('CALL db.awaitIndex(\"skill_full_text\", 300)')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.query('''\n",
    "CREATE FULLTEXT INDEX position_full_text IF NOT EXISTS FOR (n:Position) ON EACH [n.title]\n",
    "''')\n",
    "\n",
    "graph.query('CALL db.awaitIndex(\"position_full_text\", 300)')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
